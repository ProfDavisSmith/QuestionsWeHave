\chapter{Common Biases}
\label{s:biases}
For this section, we will be covering around 16 of the most common biases which people experience, but there are loads of other ones. For example, one which I have experienced is a rhyming bias. This is where you think something is true or accurate because it rhymes. This is called the Rhyme-as-Reason Effect. This is a compound of a few different facts and biases. It's pretty well been shown that people have a tendency to remember things better when they rhyme (why do you think commercial jingles and slogans work?). This means that if something rhymes, then we are more likely to remember it. Then there's the additional bias, which we will cover, which is that we think something is true because we believe/remember it. For example, "if the glove doesn't fit, you must acquit", or this children's rhyme:
\factoidbox{
   \textbf{Mrs. O'Leary's Cow}

    Late one night, when we were all in bed,\\
    Old Mother Leary left a lantern in the shed,\\
    And when the cow kicked it over, she winked her eye and said,\\
    "There'll be a HOT time on the old town tonight."\\
    FIRE, FIRE, FIRE!}

Many of us know that this is about the Great Chicago Fire, and many of us now believe that it was caused by a cow kicking over a lantern, mostly because of how easily this song was remembered. However, this is not accurate. Similarly, phrases which rhyme seem to trigger this bias without needing to be committed to memory. According to a couple studies, people tend to believe that something is more accurate when it rhymes. For example "woes unite foes" vs "tragedy unites enemies", "an apple a day keeps the doctor away" vs "an apple a day keeps you healthy", "beer before liquor, never sicker", and so on. We can avoid this bias by simply looking closer at rhyming phrases and look at why they were used, they may be there for persuasive reasons. Every bias has at least one fallacy associated with it and the fallacies get their power (we think they work or they go unnoticed) because we are under the influence of the bias. Noticing and acting against the bias is a great help in all areas of life. 

\section{Confirmation Bias}

We all have some strongly held beliefs and we like to think that these are founded on good reasons, but sometimes we accept reasons based on our previously held beliefs rather than beliefs based on reasons. Confirmation bias is where we look harder for and more easily accept evidence/reasons which confirm our beliefs when investigating a stance. Similarly, we don't look as critically at confirming data as we do disproving.  At the same time, often, this bias appears when we don't look as hard for and/or don't accept evidence/reasons which disprove our stance. I am sure you have heard of the flat-earthers or the anti-vaxxers. In both of those cases, the believers readily seek out evidence which supports the idea that the Earth is flat or that vaccination causes Autism (or some other nonsense), but are resistant to the overwhelming amount of disproving evidence.  Take Anti-vaxxers as our prime example.
\factoidbox{
    In 1998, Andrew Wakefield and a colleague published a paper which claimed to show that the MMR vaccine caused dangerous levels of certain proteins in the blood, which went into the brain causing Autism. To support this claim, Wakefield described 12 children who had developmental delay who received the MMR vaccine and were, 1 month later, diagnosed with Autism.  Groups who already had the belief, for various reasons (most, if not all wrong), that vaccines were dangerous, accepted this finding whole-hog without digging much deeper. But this is flawed on several accounts. If you dig deeper into the study, you realize that children are diagnosed with Autism around the same time as they would receive the MMR vaccine (the evidence appears at around that age). So, this does not show causation. To show causation, there would need to be a large sample size with an equal mix of children who would not receive the vaccine and those who would and then there would need to be a remarkable increase in the cases of Autism in the vaccinated children. This was not done and subsequent studies have shown that this would not happen. Additionally, looking at the 12 children which Wakefield mentioned, developmental delay is one of the early signs of Autism (though, I will say, that developmental delay does not always mean Autism), so that example does not actually support his case.}

But, all that being said, the people, with the preconceived notion that vaccines were in some way bad, did not do that digging, they just accepted the finding. Otherwise, I would hope, reasonable people would do the digging and learn more about the study, seek out ways in which the experimental parameters could have been wrong, but this confirmation bias derailed the train. The fallacy associated with this should be easy enough to see, it is where we put more weight on evidence or an argument because we agree with the conclusion or we think that an argument is less strong than it is because we disagree with the conclusion. 
\section{Self-Interest Bias}

This sort of bias is pretty easy to see in other people, especially when their behavior affects others as well as themselves. Self-interest bias is where we act or believe something merely because it benefits us (the individual) in some way, without considering the effects it might have on others. If you ever jump to a conclusion which is beneficial to you without thinking about others, then that bias has likely triggered in you. This bias causes us to overlook the interests of others and, thereby, is not a reliable guide to what is the right thing to do in most situations (involving others). Take this example:

 \factoidbox{A young man (adult) has lived and worked (on his own)  in another state from his parents for a year and, at the start of the second, started going to the local state university to get his degree. His father, when doing his taxes (for the first year of the son being away), listed the young man as a dependent. Doing so saved the father approximately \$2,000. However, when the young man applied for in-state tuition at the university, they claimed that he was a dependent of an out-of-state person, and thereby did not qualify, doubling his tuition from \$10,000 per year to \$20,000 per year.} 

The father in this case was acting in a self-interested way. If the father had confirmed with the son what listing him as a dependent would do and thought of him, then they either could have come to some sort of arrangement ("hey pa, I'll just pay you the two grand once I have myself established") or the father could have just ignored that sum. The bias made him gloss over the effects on others.

This is not to say that we should always act altruistically, that would need to be argued for in a different way (if accurate). Sometimes the right thing to do will benefit you and may even be at the expense of others. However, the bias comes into play when your reasoning does not take their interests or the effects which the actions might have on them into account at all. 

\section{Affect Bias}

This sort of bias is found in several logical fallacies. This is where you allow your emotional response to a case or situation to override you logical thinking or your actions.  For most people, the idea of the Nazis generate a strong emotional response of hatred and the idea of, say, marriage or love generates a strong positive emotional response. You can trigger this bias in another by using words, tactfully, in your argument. You could also do this to yourself, if the situation is right. For example, suppose that a person is on trial for child abuse. That context alone is enough to cause an emotional response. The evidence against the person could be outstandingly weak, but it would still be an uphill battle for them because the emotional response about such a case will have triggered this bias in your fellow jury-people and thereby give them the gut response of guilty, without even entertaining the evidence. Similarly, in extreme situations, some people have needed to resort to cannibalism to survive. Upon hearing about such a case, your gut will likely twist and made you immediately jump to the conclusion that what they did was wrong. However, this emotional response clouds the reasoning being resorting to cannibalism and makes it harder to see that the actions were for the better of the group as a whole.

This bias gets its power over us because people often think that the strength of the conviction about something (an emotional attitude towards it) is the same as the strength of the reasoning behind it. Take these two cases:
\factoidbox{\textbf{The Origami Frog Killer} 

You are the head of a detective team which has been tracking a very violent serial killer. The tabloids have dubbed this murderer “The Origami Frog Killer” after the distinctive calling card, an origami frog placed on the mangled remains of what was once the victim’s chest. Eventually, after months of effort, your team finds them. She committed suicide and left a note mocking the police force and your team.
You know that if this news came out, that you failed to bring this murderer to justice, the pain and grief felt by the populace would be extreme. There would be great distrust in the government and police force, rioting, potentially copy-cats, etc. However, there’s a fairly weak-minded individual in your holding cell for a petty crime. Your team is very loyal to you and knows that if you go down, they go down with you. Since only your team has access to the precise evidence needed to get the correct criminal, you can easily alter it, convince the person in the holding cell that they committed the crimes, and thereby frame them for it. This will prevent the grief, distrust, copy-cats, etc. which would have happened otherwise.

Do you frame the person in your holding cell?} 	
\factoidbox{\textbf{The Red Rose Gang}

You are the head of a detective team which has been trying to get a very violent gang leader behind bars. You and your team know full well that she has committed very heinous acts, including cop-killing, drug-sales, extortion, slaughter of families and children, etc. She will soon be in court for some of the crimes, but not all. The defense, however, is very crafty, and given the evidence which you could present to the court, it’s very unlikely that she will see jail-time, much less the life sentence which justly would come with the conviction. This will allow for the continuation of her activities, and, maybe, embolden her to make even more violent acts.
Your team, who think like you, will follow your orders about this case to the letter. It is perfectly within your resources to alter and forge evidence to make the case stronger and ensure that this person would never leave prison.  Doing this will prevent the continued violence which her group engages in, clean up her drug-ring, and make people less fearful of such groups, making them more likely to report them.

Do you forge the evidence?}

For the first case, The Origami Frog Killer, your immediate gut reaction is, likely, not to forge the evidence. We have a strong emotional reaction to the idea of framing an innocent person. For the second case, your reaction was likely in favor of forging the evidence. But, what is the difference? In both cases you are forging evidence and the consequences of doing so are similar (restoring faith in the justice system, preventing death and harm, etc.) Without the emotional response, your answer to the final question should be the same in both cases, either forge the evidence or not. In the second case also, this emotional response can counteract the accepted idea that people are innocent until proven guilty. The trick to avoid this bias is to recognize when your emotions have been triggered and try to suppress them.

\section{Selective-Attention Bias}

This bias is a bit different. This is where you tend to notice evidence which supports your beliefs and not noticing the ones which contradict the belief. To use Anti-vaxxers as an example again, a person might see all the evidence on both sides of the 'debate' but fail to notice the extremely vast amount of evidence which supports vaccination, turning their gaze instead to the scant few shreds of evidence which oppose vaccination. This will cause them to think that the reasoning is on their side in being opposed to vaccination, but in reality it is not. People who are swayed by this bias don't realize that we experience the world and get information through a filter. Often this filter is tuned so that it lets though the good information, confirming or comforting, and blocks the bad, contradicting or disheartening. To overcome this, you need to realize that you have a 'dog in the fight', so to speak, and pay closer attention to the evidence which your filter tries to block.

\section{Negativity Bias}

This is similar to the affect and selective-attention biases, but it does not, necessarily, involve emotions or a filter. For this bias, the tendency is to place greater weight on the negative information about something and giving less weight to the positive information about that thing. This bias can be most seen in the political arena. Political campaigns are often full of what we call 'attack ads' about the opponent. This information (or disinformation) sticks in the head because of this bias. We place a greater weight on the negative information than the positive and use that to justify our voting. Now, sometimes the negative evidence should be weighted more than the positive, but this is not because it is negative, rather because it has more relevance or pressing aspects to it. Sometimes, too, the positive information needs to be weighted more, for the same reasons. The trick to avoid this is to be impartial about the positivity or the negativity of the information given, and weight them according to relevance rather than by the kind. For example, imagine that a news headline read "Buy N Large CEO laid-off 1000 employees". This case would definitely stick in your mind and you would weigh that heavily if you are under this bias. The next headline reads "Buy N Large CEO creates 1000 new jobs in the affected areas". This directly counteracts the lay-off but, if this bias had its way, you would still think negatively of the CEO.

For another example, suppose, as is the case, that news got out that the Prime Minister of Bhutan refuses to make deals with oil and gas companies to allow them to use his country's land and resources. The news would likely paint this choice rather negatively "the PM of Bhutan refuses to allow companies into the country, stifling job growth".  The Negativity Bias would have us remember this more strongly than the news  (and facts) that Bhutan is a very small country, with a stable economy, and is the only carbon negative country in the world (not neutral, negative). They have no interest in that kind of economic growth because they already have an energy surplus which they sell to other neighboring countries. 

There's good reason we remember negatives over positives, historically, negatives were more of a threat to us and things which would save us if we remembered in the future. But, this hold-over from the past leads us more astray now than it used to.
\section{Resistance Bias}

Every last one of us can be wrong from time to time, no matter who they are or what they are doing. The more training you have in a particular field the better you are at preventing yourself from being mistaken in that field, but it's still possible. Having others around to catch our errors and correct us helps us improve and become better. However, we should not always be so self-debasing that we always change when we are told we're wrong. Sometimes, the opposing side is wrong and they are the ones who need to be corrected. There is a middle-ground which needs to be found in these cases. Resistance bias is when we are overly opposed to being corrected, even taking deep offense to the idea that we may be wrong. This causes us to miss opportunities to improve and become better. The show "The Big Bang Theory" has some great examples of this, mostly by Sheldon Cooper. In those cases, Sheldon refuses to admit that he was mistaken or just plain wrong in some case. Typically, by the end of the episode, the issue is resolved, but Sheldon's refusal to even entertain the idea that he may have been wrong is the sign of this bias. Think about how you react to criticism? Are you overly passive? Or do you just refuse to accept it all together? If it is the latter, then this bias is a strong one for you. 

The trick for this bias is to accept that you could be wrong, be humble. Accept criticism, consider it impartially. But, you need to also recognize that you could be correct. So, when you are considering the criticism, don't just show your belly and give up.

\section{Belief Bias}

This is one which I most often encounter in students when I am teaching about Ethics or arguments concerning the existence of God. Belief bias is where we judge the strength of an argument based on whether we agree with the conclusion, not on the actual logical structure or the supporting evidence. Many students come to college with the 'woke' idea that morality is in some way relative to the culture, that it depends on the beliefs of the culture and that there's no facts about morality (this is, in part, the next module, and we will, likely, see this argument again). So, what I do is spend around a week with students proving that this stance is both unsupported and, if it were true, actually quite horrifying. After giving several examples of cultures with radically different views about morality than our own (one where infanticide is fine and another where the correct funeral rights is to eat the body), I give the following argument:

\textbf{The Cultural Differences Argument}
\begin{earg}
    \item[]Cultures have different views about what is moral.
    \item[]Therefore, there is no objective morality (it's relative to the culture).
\end{earg}

If you are under the influence of this bias, then you would think that this is an extremely good argument. However, this argument is very weak. First off, the argument goes from what people believe to what is actually the case, which is a wrong move on many levels. For example, if I keep the argument structure the same and use the same kind of reasoning, some funny things pop out:

\textbf{The Shape of the Earth Argument}
\begin{earg}
    \item[]Cultures have different views about the shape of the Earth.
    \item[]Therefore, there's no objective fact about the shape of the Earth (it's relative to the culture).
\end{earg}

This argument, in form, structure, evidence, and everything else, is the same as The Cultural Differences Argument. If you thought the first argument was great but the second was loony-tunes, this bias got you.   Here are two more arguments for examples:

\begin{tabular}{p{1.5in}|p{1.5in}}
\textbf{Brain and Computer} &\textbf{Hammer and Gun}\\
\begin{earg}
    \item[]The brain is like a computer.
    \item[]They both have complex, inter-working parts and it’s very clear when one part fails to function properly.
    \item[]Yet, it would be ridiculous to claim that a computer didn’t have a designer.
    \item[]So, claiming that the brain didn’t have a designer is equally ridiculous
\end{earg}&
\begin{earg}
    \item[]A guns is like a hammer.
    \item[]They both have metal parts and could be used to kill someone.
    \item[]Yet, it would be ridiculous to restrict the purchase of hammers
    \item[]So, restrictions on purchasing guns are equally ridiculous
\end{earg}\\
\end{tabular}

Both of these arguments are very weak, if we treat them as inductive, and insanely invalid, if we treat them as deductive. However, in reading them, there's a strong likelihood that you thought that one was better than the other or you thought that both were great. Either way, this bias got you. If you thought the arguments were bad based on the conclusion alone, then this bias got you as well. The key thing about this bias is that you need to take your personal feelings out of the evaluation of the argument. My trick is to replace the thing which I agree with in the argument with something else. So, in arguments about the existence of God, I replace it with 'flying golden banana'. If the argument still has the same punch, then it works, but if it doesn't, then I know that I fell to the bias.
\section{Availability Bias}

Quick question for yourself (don't look it up): How often do people win the lottery? How frequently do celebrities get together and break-up? For the latter, I don't have the answer, but I would say not as often as you would think. For winning the lottery, it's actually very, very, rare. Lotto agencies spend a good amount of time and effort pumping our minds with examples of people winning and how awesome their lives turned out to be. This makes this evidence very available to us. Availability bias is where we are lazy and use examples which are easy or quickly come to mind. For example, the cases of cultural oppression and one culture forcing themselves on another (Cultural Imperialism) are fairly easy to come by. The easiest of them are the ones where the culture was wrong to do so. You have the actions of Spain in Latin America, the US towards Native Americans, the British in Asia (in general), and so on. These 'low hanging fruit' examples might make you think that one culture imposing its values on another is always wrong. If that's all you did, the availability bias got you. But, there are plenty of other cases where Cultural Imperialism was actually the right call. If you are of certain religious leanings, then you might think that missionaries are doing something good, but they are engaged in this behavior. For a less contentious example, any nation which sided with the Allies in WW2 were imposing their cultural beliefs on another. Going in and trying to introduce civil/human rights in a region also counts as Cultural Imperialism. These examples show that if the information is easy to come by, the conclusions drawn from them could sill be wrong. The trick with this bias, how to avoid it, is to recognize how easy the information was to get or how immediately the examples jumped to mind. The easier or the faster, the more you will need to be cautious about the conclusion drawn. Sometimes, and this is true for all of them, the bias might actually get you the right answer, but this is by luck, not by good reasoning. 

\section{Bandwagon Bias}

Come on, man, everyone is doing it. You don't want to be left out, do you? If you were right, then don't you think more would think it, too? The last three sentences are variations of things which you may have heard in the past. Peer-pressure and other things like that work because they are acting on the Bandwagon bias. This bias is where we unconsciously tend to adopt stances, beliefs, or behaviors because many other people have them or they are had by a group which we belong.  In football (either soccer or American football), we see this when people support the teams which others in their area support. This is not because, necessarily, the team is actually good but rather because other people around us support them. 'Bandwagon fans' are another example, they switch their support according to who has the most fans and who is winning. In the political arena, people often support the side which their community supports out of fear of being an 'outcast'. Alcohol commercials and tobacco advertisements (when we had them) will often show everyone drinking (or smoking) the product and having a great time. This acts on the bias because it makes us want to belong to such a group and we, unconsciously, adopt the beliefs of that group. Similarly, when I was young, riding the bus to school, the driver would blast country music over the audio system. My fellow students seemed to like it (I, frankly, did not, I grew up on heavy metal and classical). If, in this case, the bias had gotten me, I would come under the stance that country music was good music, not because of the instrumental or lyrical qualities, but because everyone else seemed to like it.

We all have the desire to make friends and belong to a group. It is the sort of beings we are. We evolved as communal and group oriented creatures. Adopting beliefs or stances on those grounds alone is simply not good enough. If you ever find yourself attracted to a stance, regardless of popularity (though this bias works best for the most popular), ask yourself about why you are drawn to the stance. Is it because your friends have it? Is it because it would get you into a group or club? Look at the stance closer and judge for yourself whether it actually holds water. For example, suppose that a young person in the Pre-Civil War South  was unsure about the morality of slavery. She might think that because her community thinks it's fine and her opposing it would leave her in financial ruin because of the community would, essentially, kick her out, that slavery must be OK.  This would not be her thinking for herself, rather it would be the lazy thing to do. Bandwagon bias, like many of the other biases, is lazy. It pushes the mental work necessary to make a reasonable conclusion off of your plate and on to the the less-reliable masses.
\section{First-Person Bias}

This is one of my favorite biases to talk about because there are some great examples of this at work in history as well as in contemporary times. First-person bias is not where you think information is more reliable because you have first person experience of it (that's another bias). This bias is where you judge what is good for others according to what is good for you. For example, some of you may have heard about the assassination of Julius Caesar. As Caesar rose to power and began to institute his reforms, a splinter political party emerged, The Liberatores. At that time, there were typically two parties, the Populares (the popular ones) and the Optimates (the best ones). The Liberatores were composed of those who believed that Caesar, through the changes, was stifling liberty and hurting the people. There are several different angles one could take on the reasoning for the assassination, none of which really support the Liberatores as being correct in their actions. What really irked them was Caesar's proposed tax plan, which would cut taxes on the farmers and poor and, in turn, increase the taxes on the wealthy (in order to improve roads, infrastructure, and so on). The Liberatores (changing a few things here for my example) were very rich and this tax policy would hurt them. If it hurts us, they thought, it must hurt everyone else. Upon killing Caesar, they left and found that the masses were far from happy, Caesar's policies would have helped them immensely. The Liberatores failed to see this because of this first person bias. 

For another, suppose that you, wrongly, really dislike the rye chips in the snack mixes. A coworker offers some to you, so you, thinking that you were doing right by them, pick out the chips for yourself, saving them the 'gross parts'. They reply to this with something along the lines of 'man, you took the best part!' Here, you evaluated the situation about the tastiness of the snack mix according to your own preferences and did not take their preferences into account.

A common result of this bias is sort of the opposite of the negativity bias. Because of this bias, sometimes, we place a higher (unwarranted) weight on what is good for ourselves and place a lower (unwarranted) weight on what is bad for others. With the Liberatores, they placed a higher weight on what was good for them and did not factor in (or didn't factor in as much as they should have) the negatives on other people. This has resulted, and has continued to cause, many social injustices in the world. Different groups and communities face different struggles and what is good for one group (such as maintaining a status quo) could add to (or continue) those struggles. The main take-away from this is that to avoid this bias, you need to meet each other on the level, put yourself in their shoes, recognize that what is good for you might not be good for them.

\section{Ethnocentricim and Stereotyping}
This is most closely related to bandwagon biases. It is one of the root causes for racism and many of the struggles which we face today. This is the irrational belief that your ethnic group, society, or culture in innately or fundamentally superior to other groups. This, along with certain other biases, causes the person under its sway to see their group in the best possible light while seeing other groups in the worst possible light. Thus the bias will cause you to fail to see some relevant facts on both sides. Ethnocentrism can lead people to think that an individual is guilty based on the color of their skin or their religion. This bias has lead to a great deal of suffering which is still happening to this day.

Related to this is stereotyping. Now, we can't hold all of the information about a thing in our heads immediately. We just don't have the memory capacity for it. So, we, naturally, simplify things and place it into categories. These categories tend to be very simple and, for most things, tend to be fairly neutral. For example, if I present you with a picture of a car, you will quickly put it in the category 'car' and be able to make various assumptions about the car based on the information in the category. This is easy and very efficient and, for many things, it will not lead you too far astray. However, sometimes nasty aspects will infect your categories. These might be positive or they might be negative. In either case, it removes the neutrality of the category. This is where we get stereotyping. They are oversimplified and, typically, unflattering, generalizations on members of a group. For a first example, there are many people who play World of Warcraft. Picture in your mind what you think a player would be. For many, they are unhealthy, socially awkward, and otherwise unpleasant. Those are stereotypical features of a WoW player. Ethnocentrism will cause you, also, (if you aren't a player) to paint this group in that negative aspect and re-enforce the idea. For another example, suppose that you have in your mind that professors with certain accents aren't as smart as professors with a different accent. This will lead you to stereotype the professor upon hearing their accent and thereby start employing the confirmation bias to support that claim (that the professor isn't as smart). This will re-enforce the idea in your mind that the generalization is correct and deeper ingrain it into your mind. In Ethnocentrism, we get that the nasty pollutants in the categories are both positive things for your own group and negative aspects for others.\footnote{Much of the content here and explanations of stereotyping came from an application of the theories and information in Mental Files by François Recanati and Davis Smith's (my) Master's Thesis "Here Be Dragons".}

Avoiding both of these things is a great struggle in this day and age. We see regularly the effects of Ethnocentrism and stereotyping, all not good. You need to analyses your categories, see what is in them, and recognize when these features are unwarranted. This is especially true for groups of people. Never include an evaluative judgement in such a category, whether it is the category for your group or the group of another.

\section{False Consensus Bias}

This is another bias which is most often found in the political arena but it can also be seen in other aspects. You may have heard people claim that if you leave a certain bubble, then you will see that very few actually hold the same stance as you. For example, I live in a fairly liberal leaning state, no denying that (given voting patterns and surveys), but people I know claim that if I were to really check the figures, then I would find that most people in this state are actually conservative. I only ever hear this from conservatives and this is an example of the bias which I am talking about here. The false consensus bias is where you think that most people believe the same things as you merely because you believe it. You don't confirm your hunch or check the real data. It is equally possible for left-leaning people to fall for this trap as well. For example, suppose that a person believes that most people believe that there should be far more gun control. The bias comes out when they believe this without checking the surveys. The consequences of this line of thinking can be found in the way certain groups receive their news. Some person posts a theory or a 'fact' (may or may not be accurate), this gets picked up by another group, citing them for credibility, and then on and on it goes, leading to an echo-chamber, making people believe that more and more people agree with them. Conspiracy theory forums are a great example of this, same with other propaganda sources. The belief that many agree with you can also cause you to fall into the bandwagon bias which we saw before, and further re-enforce the belief, without actually adding any real evidence to it.

To avoid this bias, you need to be careful about what you think others believe. This solution is similar, in sorts, to the first person bias. Check to see whether many people agree with you, trust surveys and statistics, or, better yet, learn how to create impartial surveys and conduct them yourself.   You may find that most people don't necessarily agree with you. This can be good. Figure out why others disagree with you, what is their reason. You might find that you were the mistaken one.
\section{Externalization Bias}

We have all made mistakes from time to time, it's part of the human condition and it's how we grow and become better. Learning from our mistakes is a key part of life. However, some people will look elsewhere for who is at fault, they will not look at themselves. This is the externalization bias. For example, suppose that I got into a car accident after running a red light. I could blame myself, say that I was distracted while driving and grow from the experience. Or, and this is where I would have the bias, I could blame the light, the other drivers, and anything but myself. I could say that I was sure the light was yellow or green and claim that the light must have switched the other side to green early. How would I grow and learn from my mistake?

Externalization bias is where we, for various reasons, refuse to take on the responsibility for our actions. Something external to us is causing the issues, not us. We make up excuses. We see this both on an individual level as well as in a group setting. For example, suppose that you are in a competition to build a rocket engine, the one which produces the most power wins. The other teams have far more experience in a competition setting and this is your team's first year in the competition (though you have the same level of experience in building engines). When you don't win, you could blame the other teams, claim that the judges were in some way partial towards them, or say that there was some kind of sabotage (without evidence). This would be the externalization bias at work. On the other hand, you can learn from your errors, figure out what made their engine better, how you can improve your designs, and see that you, as a team, grow from the experience.

Sometimes, however, the issue is external to us. Sometimes, something is wrong with the system. So, you should not always place blame on yourself for a failure. The trick is to fully analyze what you did, learn from that, and then, if everything was perfect, blame something external. 

\section{Substitution Bias}

We have all encountered questions which are hard to answer or understand. Often, without realizing it, we will replace or omit key words or phrases in the questions with others, making the question easier to answer, thinking that we answered the original.  But, often, replacing those key words or phrases in the question makes it a new question all together and makes your answer irrelevant to the original question. For example, take this question and answer pair:

\begin{earg}
\item[Question]What do you think of the nuclear triad? 
\item[Answer]Nuclear weapons are powerful and devastating in their effectiveness and power. There needs to be safe-guards to ensure that no bad people can get their hands on them.
\end{earg}

If you know anything about the US nuclear weapons capabilities, then you know that this answer makes little to no sense. It completely dodges the question. The nuclear triad is a tree-part (triad) military structure which consists of ways in which the US can launch nuclear weapons around the globe. It can be from the land, sea, or air. Reasonable responses to this question include things about whether the structure limits the chances of an accidental firing, whether the structure actually ensures the safety of the country in question, whether the hardware/software involved in launching the nuclear weapons is up-to-date or need to be improved, whether the command structure is efficient enough (or too efficient), etc.

But all of those answers require that the respondent know about the details of the nuclear triad and has thought deep enough about the structure to actually hold an opinion about it (aside from the hardware/software aspects, I know very little about the structures within the triad, so I would need to do research and form an opinion). Those answers are hard to accurately come up with. Intellectual laziness is at fault again when we use this. By just replacing the word 'triad' with 'weapons' in the question, we get a far easier question which can be very simply answered. But, that question does not relate back to the context or the relevant aspects of the original. 

For another example, think about this question answer pair:
\begin{earg}
\item[Question] What do you think about the impact of organic farming on global food production?
\item[Answer] I think it's great that people have the ability to buy organic foods and be sure that their foods are safe for their families, without pesticides and the like.
\end{earg}
In this case, the question was not about the availability or production of organic foods. Correct responses to this question include a stance about the average cost of producing food, the amount of land required to grow organically (it is far greater), the response of food producers to the wave of demand for organic food, and so on. But again, all of these require a good amount of research and the in depth knowledge about the impacts of organic farming, and it requires you to also have confidence in that research to form an opinion. All of those are missing from the answer. Again this is intellectual laziness at work.

Avoiding this involves checking your answer before you speak/write it. Did you gloss over a word in the question? Did you miss something which should have been there? I have encountered hundreds of papers now where a student wrote at length about something totally different than what was expected. Always check yourself.
\section{Anchoring Bias}

An anchor in this context, is a fixed point, something which you build off of or around for a given topic. Sometimes, this anchor can be good, a reasonable starting place for building an understanding on a topic. But other times, this anchor could be placed in the wrong spot. This anchor is typically the first thing you learn about a topic and it shapes or colors your opinions/attitudes (whether you accept or reject) new information about it. For example, many people first learn about the origin of the Earth and humans from religious institutions, like churches. In the Christian stories, it involves God creating everything in 7 days. For some, this is their anchor about the origin of the world. The anchoring bias is when one refuses to recognize that their foundational understanding of the topic was mistaken or that this, often mistaken, understanding colors all other learning about the topic. Some Creationist thinkers could be accused of having this bias. They set their anchor about the origins of humans on their first encounter and refuse to budge from that position, clouding their understanding of the rest of the data. This is often coupled with confirmation biases as well as negativity biases to further dig the anchor in. You place higher weight on this first chunk of information than warranted.

Similarly, this can come in the form of an attitude. Think about a person who shapes their opinions of others based on a core anchor. They could have the idea 'if they belong to the Flat Earth Society of America then they aren't smart, otherwise they're good'. What would you think of such a person? The anchoring bias has given them a far too granular sense of the world. In this sense, the anchoring bias gives an overly simplistic and inaccurate scale to gauge people on. 

Avoiding cases like this involves understanding that all parts of your understanding could be mistake. Look at the reliability of the first thing you learned? Did it come from an actually reliable source? Has new information come out to contradict that stance? Treat the first thing you learned the same as you would anything else. Have a good amount of skepticism when it comes to this.
